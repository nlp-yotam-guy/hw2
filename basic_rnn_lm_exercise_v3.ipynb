{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic_rnn_lm_exercise_v3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "P8MdOYnw4sev",
        "colab_type": "code",
        "outputId": "1d9caccc-2c13-41da-fd5c-308d2c656ebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "cell_type": "code",
      "source": [
        "#RUN ME FIRST (ONCE)\n",
        "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
        "!tar -xzf simple-examples.tgz\n",
        " "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-23 13:26:16--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
            "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
            "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34869662 (33M) [application/x-gtar]\n",
            "Saving to: ‘simple-examples.tgz.7’\n",
            "\n",
            "simple-examples.tgz 100%[===================>]  33.25M  2.91MB/s    in 13s     \n",
            "\n",
            "2019-04-23 13:26:29 (2.58 MB/s) - ‘simple-examples.tgz.7’ saved [34869662/34869662]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uCz3Z-WVz5wl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook\n",
        "import pandas as pd\n",
        "import matplotlib \n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import os\n",
        "\n",
        " \n",
        "\"\"\"Utilities for parsing PTB text files.\"\"\"\n",
        "def _read_words(filename):\n",
        "  with tf.gfile.GFile(filename, \"r\") as f:\n",
        "    return f.read().replace(\"\\n\", \"<eos>\").split()\n",
        " \n",
        " \n",
        "def _build_vocab(filename):\n",
        "  data = _read_words(filename)\n",
        " \n",
        "  counter = collections.Counter(data)\n",
        "  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
        " \n",
        "  words, _ = list(zip(*count_pairs))\n",
        "  word_to_id = dict(zip(words, range(len(words))))\n",
        " \n",
        "  return word_to_id\n",
        " \n",
        " \n",
        "def _file_to_word_ids(filename, word_to_id):\n",
        "  data = _read_words(filename)\n",
        "  return [word_to_id[word] for word in data]\n",
        " \n",
        " \n",
        "def ptb_raw_data(data_path=None):\n",
        "  \"\"\"Load PTB raw data from data directory \"data_path\".\n",
        " \n",
        "  Reads PTB text files, converts strings to integer ids,\n",
        "  and performs mini-batching of the inputs.\n",
        " \n",
        "  The PTB dataset comes from Tomas Mikolov's webpage:\n",
        " \n",
        "  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
        " \n",
        "  Args:\n",
        "    data_path: string path to the directory where simple-examples.tgz has\n",
        "      been extracted.\n",
        " \n",
        "  Returns:\n",
        "    tuple (train_data, valid_data, test_data, vocabulary)\n",
        "    where each of the data objects can be passed to PTBIterator.\n",
        "  \"\"\"\n",
        " \n",
        "  train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
        "  valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
        "  test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
        " \n",
        "  word_to_id = _build_vocab(train_path)\n",
        "  train_data = _file_to_word_ids(train_path, word_to_id)\n",
        "  valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
        "  test_data = _file_to_word_ids(test_path, word_to_id)\n",
        "  vocabulary = len(word_to_id)\n",
        "  return train_data, valid_data, test_data, vocabulary\n",
        " \n",
        " \n",
        "def ptb_iterator(raw_data, batch_size, num_steps):\n",
        "  \"\"\"Iterate on the raw PTB data.\n",
        " \n",
        "  This generates batch_size pointers into the raw PTB data, and allows\n",
        "  minibatch iteration along these pointers.\n",
        " \n",
        "  Args:\n",
        "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
        "    batch_size: int, the batch size.\n",
        "    num_steps: int, the number of unrolls.\n",
        " \n",
        "  Yields:\n",
        "    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n",
        "    The second element of the tuple is the same data time-shifted to the\n",
        "    right by one.\n",
        " \n",
        "  Raises:\n",
        "    ValueError: if batch_size or num_steps are too high.\n",
        "  \"\"\"\n",
        "  raw_data = np.array(raw_data, dtype=np.int32)\n",
        " \n",
        "  data_len = len(raw_data)\n",
        "  batch_len = data_len // batch_size\n",
        "  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
        "  for i in range(batch_size):\n",
        "    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
        " \n",
        "  epoch_size = (batch_len - 1) // num_steps\n",
        " \n",
        "  if epoch_size == 0:\n",
        "    raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
        " \n",
        "  for i in range(epoch_size):\n",
        "    x = data[:, i*num_steps:(i+1)*num_steps]\n",
        "    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
        "    yield (x, y)\n",
        "    \n",
        "DATA_PATH = \"simple-examples/data\" \n",
        "HIDDEN_SIZE = 256          \n",
        "NUM_LAYERS = 1\n",
        "VOCAB_SIZE = 10000         \n",
        "                             \n",
        "\n",
        "LEARNING_RATE = 1 \n",
        "TRAIN_BATCH_SIZE = 100     \n",
        "TRAIN_NUM_STEP = 20       \n",
        "\n",
        "\n",
        "EVAL_BATCH_SIZE = 100      \n",
        "EVAL_NUM_STEP = 1           \n",
        "NUM_EPOCH = 20\n",
        "KEEP_PROB = 0.65\n",
        "MAX_GRAD_NORM = 5   \n",
        "\n",
        "\n",
        "def add_placeholders(batch_size, num_steps):\n",
        "    \"\"\"Generates placeholder variables to represent the input tensors.\n",
        "\n",
        "    These placeholders are used as inputs by the rest of the model and will be \n",
        "    fed data during training. \n",
        "    Hint: You might find tf.placeholder useful.\n",
        "\n",
        "    Returns:\n",
        "      input_data: Input placeholder tensor of  shape (batch_size, num_step), type tf.int32\n",
        "      targets: Labels placeholder tensor of shape (batch_size, num_steps), type tf.int32\n",
        "\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE (~2-3 lines)\n",
        "    input_data = tf.placeholder(tf.int32,[batch_size, num_steps])\n",
        "    targets = tf.placeholder(tf.int32,[batch_size, num_steps])\n",
        "    return input_data,targets\n",
        "    ### END YOUR CODE\n",
        "\n",
        "\n",
        "def add_rnn_cell(HIDDEN_SIZE,batch_size,KEEP_PROB,is_training,num_steps):\n",
        "    \"\"\"\n",
        "    TODO: In the code below, \n",
        "    - define the recurrent cell using tf.nn.rnn_cell.LSTMCell\n",
        "       and apply dropout using tf.nn.rnn_cell.DropoutWrapper at training time.\n",
        "    - define the initial state for the cell.\n",
        "    Hint: Each LSTMCell instance has a zero_state method.\n",
        "\n",
        "    Args:\n",
        "      - HIDDEN_SIZE - the size of the hidden layer.\n",
        "      - batch_size - the size of the batch\n",
        "      - KEEP_PROB - the probability we will not apply dropout to a neuron. \n",
        "      - VOCAB_SIZE - the size of the vocabulary.\n",
        "      - is_training - boolean to indicate if we are training or evaluating.\n",
        "      - num_steps - the length to unroll the RNN.\n",
        "    Returns:\n",
        "    - lstm_cell - a lstm cell with HIDDEN_SIZE neurons.\n",
        "    - initial_state - the zero state for the cell \n",
        "\n",
        "    \"\"\"      \n",
        "    ### YOUR CODE HERE (~3-4 lines)\n",
        "    print('r')\n",
        "    lstm_cell = tf.nn.rnn_cell.LSTMCell(HIDDEN_SIZE)\n",
        "    if is_training:\n",
        "      lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell,output_keep_prob=KEEP_PROB)\n",
        "    cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell]*NUM_LAYERS)\n",
        "    initial_state = cell.zero_state(batch_size,tf.float32)\n",
        "    return cell,initial_state\n",
        "    ### END YOUR CODE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def add_embedding_op(input_data,VOCAB_SIZE, HIDDEN_SIZE,KEEP_PROB,is_training):\n",
        "    \"\"\"\n",
        "\n",
        "    TODO: In the code below, \n",
        "    - Create a [VOCAB_SIZE, HIDDEN_SIZE] matrix using tf.get_variable to\n",
        "      represent the embedding of our vocabulary\n",
        "    - Read the embeddings of the input. \n",
        "    - Apply dropout to the inputs at training time.\n",
        "\n",
        "    Hint: You might find the following methods helpful: tf.get_variable, \n",
        "    tf.nn.dropout and tf.nn.embedding_lookup \n",
        "\n",
        "    Args:\n",
        "      - input_data - a tensor of size [batch_size, num_steps].\n",
        "      - VOCAB_SIZE - the size of the vocabulary.\n",
        "      - HIDDEN_SIZE - the size of the hidden layer.\n",
        "      - KEEP_PROB - the probability we will not apply dropout to a neuron. \n",
        "      - is_training - boolean to indicate if we are training or evaluating.\n",
        "    Returns:\n",
        "      - inputs - a tensor of size (batch_size, num_steps, HIDDEN_SIZE) representing\n",
        "        the embedding for the current inputs.\n",
        "\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE (~3-4 lines)\n",
        "    with tf.variable_scope(\"add_embedding_op\", reuse=tf.AUTO_REUSE):\n",
        "      embeddings = tf.get_variable(\"embeddings\",[VOCAB_SIZE, HIDDEN_SIZE])\n",
        "    inputs = tf.nn.embedding_lookup(embeddings,input_data)\n",
        "    if is_training:\n",
        "      inputs = tf.nn.dropout(inputs,KEEP_PROB)\n",
        "    return inputs\n",
        "    ### END YOUR CODE\n",
        "\n",
        "\n",
        "def add_prediction_op(cell,inputs,initial_state,VOCAB_SIZE):\n",
        "    \"\"\" \n",
        "    TODO: In the code below, \n",
        "      - use tf.nn.static_rnn with the initial_state.\n",
        "      - return the final state from tf.nn.static_rnn.\n",
        "      - apply a dense layer with VOCAB_SIZE units on the output from \n",
        "        tf.nn.static_rnn and return the logits.\n",
        "    Args:\n",
        "      - cell - the rnn cell.\n",
        "      - inputs - a tensor of size (batch_size, num_steps, HIDDEN_SIZE).\n",
        "      - initial_state - the initial state of the rnn cell.\n",
        "\n",
        "    Returns:\n",
        "      - logits - the result of the dense layer.\n",
        "      - state - the final_state of the rnn.\n",
        "\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE (~3-4 lines)\n",
        "    print('shape: ',inputs.shape)\n",
        "    t = tf.cast(inputs,tf.float32)\n",
        "    t = tf.unstack(t,axis=1)\n",
        "    #print('len: ',len(t),' shape: ',t[0].shape)\n",
        "    output,final_state = tf.nn.static_rnn(cell,t,initial_state)\n",
        "    output = tf.stack(output,axis=1)\n",
        "    logits = tf.layers.dense(output,VOCAB_SIZE)\n",
        "    return logits, final_state\n",
        "    ### END YOUR CODE\n",
        "\n",
        "\n",
        "def add_loss_op(logits,labels,batch_size):\n",
        "    \"\"\" Defines the CE loss operation within the computation graph and return the perplexity\n",
        "\n",
        "    TODO: In the code below, \n",
        "      - Use tf.nn.sparse_softmax_cross_entropy_with_logits to define your loss.\n",
        "      - Calculate the perplexity w.r.t the batch size.\n",
        "\n",
        "    Args:\n",
        "      - logits - the result of the dense layer in the graph\n",
        "      - labels - the correct labels for the next word in sentence as indices in \n",
        "        the vocabulary.\n",
        "      - batch_size - the size of the batch.\n",
        "    Returns:\n",
        "      - loss - the perplexity for the current batch.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE (~2-4 lines)\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
        "    cost = tf.reduce_sum(loss)/batch_size\n",
        "    #perp = tf.exp(loss)\n",
        "    print('perp: ',cost)\n",
        "    return cost\n",
        "    ### END YOUR CODE\n",
        "\n",
        "\n",
        "class RNNModel(tf.keras.Model):\n",
        "    def __init__(self, is_training, batch_size, num_steps):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.batch_size = batch_size \n",
        "        self.num_steps = num_steps \n",
        "        \n",
        "        self.input_data,self.targets = add_placeholders(self.batch_size, self.num_steps) \n",
        "        \n",
        "        self.cell,self.initial_state =  add_rnn_cell(HIDDEN_SIZE,batch_size,KEEP_PROB,is_training,self.num_steps)\n",
        "        \n",
        "        self.inputs = add_embedding_op(self.input_data,VOCAB_SIZE, HIDDEN_SIZE,KEEP_PROB,is_training)\n",
        "        print( self.inputs)\n",
        "        self.logits,self.final_state = add_prediction_op(self.cell,self.inputs,self.initial_state,VOCAB_SIZE)\n",
        "        \n",
        "        self.loss = add_loss_op(self.logits,self.targets,self.batch_size)\n",
        "\n",
        "       \n",
        "        if not is_training: return\n",
        "\n",
        "        trainable_variables = tf.trainable_variables()\n",
        "        \n",
        "        grads, _ = tf.clip_by_global_norm(\n",
        "            tf.gradients(self.loss, trainable_variables), MAX_GRAD_NORM)\n",
        "\n",
        "       \n",
        "        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
        "\n",
        "     \n",
        "        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables))\n",
        "      \n",
        "\n",
        "\n",
        "def run_epoch(session, model, data, train_op, output_log,pbar=None):\n",
        "   \n",
        "    total_loss = 0.0\n",
        "    iters = 0\n",
        "    state = session.run(model.initial_state)\n",
        "    prep_list=[]\n",
        "    for step, (x, y) in (enumerate(\n",
        "        ptb_iterator(data, model.batch_size, model.num_steps))):\n",
        "        \n",
        "        loss,state,_ = session.run(\n",
        "            [model.loss,model.final_state, train_op],\n",
        "            {model.input_data: x, model.targets: y,model.initial_state:state})\n",
        "       \n",
        "        total_loss += loss\n",
        "        iters += model.num_steps\n",
        "        if pbar:\n",
        "          prep=np.exp(total_loss / iters)\n",
        "          pbar.set_description(\"%.3f\" % (prep))\n",
        "          prep_list.append(prep)\n",
        "    return np.exp(total_loss / iters),prep_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eJvfD0_KNPHs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "2c3a27b2-fba9-488d-e6a8-8b30364594ca"
      },
      "cell_type": "code",
      "source": [
        "#main cell\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "def main():\n",
        "    tf.random.set_random_seed(1234)\n",
        "    train_data, valid_data, test_data, _ = ptb_raw_data(DATA_PATH)   \n",
        "    initializer = tf.glorot_uniform_initializer(seed=1234)\n",
        "    with tf.variable_scope(\"language_model\", \n",
        "                              reuse=None, initializer=initializer):\n",
        "        train_model = RNNModel(True, TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)\n",
        "    with tf.variable_scope(\"language_model\", \n",
        "                              reuse=True, initializer=initializer):\n",
        "        eval_model = RNNModel(False, EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
        "    pr_list=[]\n",
        "    with tf.Session() as session:\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "        pbar=tqdm_notebook(range(NUM_EPOCH))\n",
        "        for i in pbar:\n",
        "\n",
        "\n",
        "            pr,_=run_epoch(session, train_model, \n",
        "                        train_data, train_model.train_op, True,pbar=pbar)\n",
        "            pr_list.append(pr)\n",
        "            if (i+1)%5==0:\n",
        "              valid_perplexity,_ = run_epoch(\n",
        "                 session, eval_model, valid_data, tf.no_op(), False)\n",
        "              print(\"Epoch: %d Validation Perplexity: %.3f\" % ( \n",
        "                      i + 1, valid_perplexity))\n",
        "        pbar.close()\n",
        "        print(\"Finished training\")\n",
        "    print(\"Epoch: %d Validation Perplexity: %.3f\" % ( \n",
        "        i + 1, valid_perplexity))\n",
        "    return pr_list\n",
        "pr_list=main()\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r\n",
            "WARNING:tensorflow:From <ipython-input-1-72b657528bd2>:163: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-1-72b657528bd2>:166: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-1-72b657528bd2>:202: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Tensor(\"language_model/dropout/mul:0\", shape=(100, 20, 256), dtype=float32)\n",
            "shape:  (100, 20, 256)\n",
            "WARNING:tensorflow:From <ipython-input-1-72b657528bd2>:229: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From <ipython-input-1-72b657528bd2>:231: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "perp:  Tensor(\"language_model/truediv:0\", shape=(), dtype=float32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "r\n",
            "Tensor(\"language_model_1/embedding_lookup/Identity:0\", shape=(100, 1, 256), dtype=float32)\n",
            "shape:  (100, 1, 256)\n",
            "perp:  Tensor(\"language_model_1/truediv:0\", shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c45ccaec808a4e538148ef43144d9fd2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=20), HTML(value=u'')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5 Validation Perplexity: 139.913\n",
            "Epoch: 10 Validation Perplexity: 112.610\n",
            "Epoch: 15 Validation Perplexity: 106.590\n",
            "Epoch: 20 Validation Perplexity: 104.709\n",
            "\n",
            "Finished training\n",
            "Epoch: 20 Validation Perplexity: 104.709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Un0q2wwMDShw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "a2a90237-5e59-4fd6-9396-4141030d6f3b"
      },
      "cell_type": "code",
      "source": [
        "def plot_perp(pr_list):\n",
        "## YOUR PLOTING CODE HERE\n",
        "  import matplotlib.pyplot as plt\n",
        "  x = np.array(range(1,len(pr_list)+1))\n",
        "  plt.plot(x,pr_list)\n",
        "  plt.title('perplexity per epoch')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('perplexity')\n",
        "  plt.show()\n",
        "  return\n",
        "## END YOUR PLOTTING COD\n",
        "plot_perp(pr_list)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMwsLScIMzNLywnCzMzScoIwM7O0/j87fUjGYo4W3QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}